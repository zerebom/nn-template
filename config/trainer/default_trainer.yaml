trainer:
  gpus: ${data.gpus}
  # distributed_backend: ddp
  accumulate_grad_batches: 1
  profiler: False
  max_epochs: 500
  log_save_interval: 100
  gradient_clip_val: 0.5
  num_sanity_val_steps: 0
  weights_summary:
